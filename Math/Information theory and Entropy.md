#### 文章目录
- [1.自信息与熵](#1)  
    - [1.1 自信息](#1.1)
    - [1.2 熵](#1.2)
    - [1.3 相对熵(KL散度)](#1.3)
    - [1.4 JS散度](#1.4)
    - [1.5 交叉熵](#1.5)
- [2. 联合自信息与联合熵](#2)
    - [2.1 联合自信息](#2.1)
    - [2.2 联合熵](#2.1)
- [3. 条件自信息与条件熵](#3)
    - [3.1 条件自信息](#3.1)
    - [3.2 条件熵](#3.2)
- [4. 互信息](#4)
    - [4.1 互信息](#4.1)
    - [4.2 平均互信息](#4.2)
    - [4.3 KL散度与平均互信息](#4.3)
- [5. 关系总结](#5)
![关系总结](https://github.com/Catalyst0307/Pictures/blob/main/84442ad5932a4f919b6f2811eef9465d.png "关系总结") 


# <span id="1">1. 自信息与熵</span>
## <span id="1.1">1.1 自信息</span>
  概率论中，随机事件是样本空间的子集。利用信息论可以对事件所包含的信息进行量化，得到信息量。事件所包含的不确定性越高，所含的信息量越大。  
  假设![](https://latex.codecogs.com/svg.image?P(x_i))表示`x_i`事件发生的概率`I(x_i)`表示事件x_i包含的信息量，则I(x_i)与p（x_i）的关系反映如下规律：  
  1. I（x_i）与P（x_i）为函数关系  
    即 `I（x_i）=f[P(x_i)]`  
  2. P(x_i) 与 I(x_i)为负相关  
  3. 若干相互独立事件所含信息量为各独立事件信息量的和  
    即 `I(x1+x2+...)=I(x1)+I(x2)+I(x3)+...`  
      
为满足上述规律，定义事件X = x_i的信息量或自信息为概率的负对数：  
    I（x_i）= -log_2P(x_i)  
    
  - 信息量的单位为比特(bit), 1bit对应P（x_i）= 1/2。每一个二进制码的信息量为1bit。对数的底数决定信息量单位，当底数为_e_时单位为奈特(nit); 当底数为10时单位为哈特莱(Hartly)。  
  - 自信息的含义有两种：（1）当事件发生前，事件的不确定性；（2）事件发生后，事件所包含的信息量。  

  - 除此之外，自信息还可描述`惊奇程度`  

## <span id="1.2">1.2 熵</span>


## <span id="1.3">1.3 相对熵(KL散度)</span>


## <span id="1.4">1.4 JS散度</span>


## <span id="1.5">1.5 交叉熵</span>


# <span id="2">2. 联合自信息与联合熵</span>


## <span id="2.1">2.1 联合自信息</span>


## <span id="2.2">2.2 联合熵</span>


# <span id="3">3. 条件自信息与条件熵</span>


## <span id="3.1">3.1 条件自信息</span>


## <span id="3.2">3.2 条件熵</span>


# <span id="4">4. 互信息</span>


## <span id="4.1">4.1 互信息</span>


## <span id="4.2">4.2 平均互信息</span>


## <span id="4.3">4.3 KL散度与平均互信息</span>


# <span id="5">5. 关系总结</span>
